{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkFdE1m3Sl7w"
   },
   "source": [
    "# **Recommendation of points of interest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS7Qv0nCSvk7"
   },
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Following code uses Gowalla data set (\"https://snap.stanford.edu/data/loc-gowalla.html\"). The dataset includes 36,001,959 visits made by 407,533 users in 2,724,891 POIs. This data covers in particular the locations of users' visits as well as their social networks (or \"friends\"). This data is used to recommend places to travel next to the user. \n",
    "\n",
    "The recommendations are generated in following 4 ways - \n",
    "1) Highest rated by all the users (popular places to visit)\n",
    "2) Highest rated by friends\n",
    "3) Highest rated by user themselves in past\n",
    "4) A collaborative approach, originally presented in: \"iGSLR: Personalized Geo-Social Location Recommendation: A Kernel Density Estimation Approach\", Zhang and Chow, SIGSPATIAL'13 (\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.701.814&rep=rep1&type=pdf\")\n",
    "\n",
    "The 4th method uses a combination of social networks of the user and the relation of the geographical location of places visited in the past and recommended place to calculate ratings. The code for the 4th method is based on the linked paper, and from this blog post (\"https://towardsdatascience.com/where-to-travel-next-a-guide-to-building-a-recommender-system-for-pois-5116adde6db\")\n",
    "\n",
    "\n",
    "The code is implemented using **Surprise** library (http://surpriselib.com/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing all the dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the important libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic \n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from surprise import AlgoBase\n",
    "from surprise import PredictionImpossible\n",
    "from itertools import combinations\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise.accuracy import rmse\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXspKeHGTVT0"
   },
   "source": [
    "# 2. Data loading and processing\n",
    "\n",
    "In order to process and load the data, perform the following steps:\n",
    "* Extract the dataset and load it into a *pandas* *dataframe*.\n",
    "* The data is extracted from 2 csv files (\"Gowalla_totalCheckins.csv\" and \"Gowalla_edges.csv\") and loaded into 2 dataframes (df_totalCheckins and df_edges) from which 4 data frames are made (df_checkins, df_friendship, df_locations, df_userinfo) which are further filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_totalCheckins = pd.read_csv(\"./Gowalla_totalCheckins.csv\", names=[\"userid\", \"dateTime\", \"lat\", \"lng\", \"placeid\"])\n",
    "df_edges = pd.read_csv(\"./Gowalla_edges.csv\", names=[\"userid1\", \"userid2\"])\n",
    "# print(df_totalCheckins[0:3])\n",
    "# print(df_edges[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_checkins = df_totalCheckins[[\"userid\", \"placeid\"]].copy(deep=True).drop_duplicates(inplace=False,ignore_index=True) #to remove duplicate rows\n",
    "df_checkins = df_totalCheckins[[\"userid\", \"placeid\"]].copy(deep=True) \n",
    "#deep= True so that changes in the df_checkins won't be done in df_totalCheckins\n",
    "# print(df_checkins[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_friendship = df_edges.copy(deep=True)\n",
    "# print(df_friendship[0:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations = df_totalCheckins[[\"placeid\", \"lng\", \"lat\"]].copy(deep=True).drop_duplicates(inplace=False,ignore_index=True) #to remove duplicate rows\n",
    "# df_locations = df_totalCheckins[[\"placeid\", \"lng\", \"lat\"]].copy(deep=True)\n",
    "df_locations.columns=[\"id\", \"lng\", \"lat\"]\n",
    "# print(df_locations[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_userinfo = df_totalCheckins[[\"userid\"]].copy(deep=True).drop_duplicates(inplace=False, ignore_index=True)\n",
    "# df_userinfo = df_totalCheckins[[\"userid\"]].copy(deep=True)\n",
    "df_userinfo.columns=[\"id\"]\n",
    "# print(df_userinfo[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a small dataset is taken because of computing reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checkins = df_checkins.head(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jJIGFwuad-b"
   },
   "source": [
    "*   Remove users who have made less than 5 visits or more than 50 visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of checkins for each user (for all places)\n",
    "df_grouped_checkins  = df_checkins.groupby(['userid'] , as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover only the userid with more than 5 places visited and it is also necessary to reduce to less than 50 places because of the complexity\n",
    "filtered_user_ids = df_grouped_checkins[(df_grouped_checkins.placeid >= 5) & (df_grouped_checkins.placeid <= 50)].userid.values \n",
    "# filtered_user_ids = df_grouped_checkins[(df_grouped_checkins.placeid > 0)].userid.values \n",
    "df_filtered_checkins = df_checkins[df_checkins.userid.isin(filtered_user_ids)]\n",
    "# print(df_filtered_checkins[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dtaframe df_friendship too\n",
    "df_filtered_friendship = df_friendship[df_friendship.userid1.isin(filtered_user_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dtaframe userinfo too\n",
    "df_filtered_userinfo = df_userinfo[df_userinfo.id.isin(filtered_user_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAkhmCM1aeBV"
   },
   "source": [
    "*   Associate each user with their list of friends and place the result in a *dataframe*: *df_user_friends*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_friends = df_friendship.groupby('userid1').userid2.apply(list).reset_index(name='friends_list') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u7g9w5IaeE_"
   },
   "source": [
    "* Calculate the frequency of each pair *(user, POI)* and put the result in a *dataframe* *df_frequencies*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df_filtered_checkins with df_locations using place_id and id fields\n",
    "#following is a left outer join=> use keys (not cols) from left table only=> only places that r in left table make to final table \n",
    "df_checkins_locations = pd.merge(df_filtered_checkins, df_locations,left_on=\"placeid\",right_on=\"id\",how=\"left\") \n",
    "# print(df_filtered_checkins[0:20])\n",
    "# print(df_checkins_locations[0:10])\n",
    "df_checkins_locations = df_checkins_locations.dropna() \n",
    "# print(df_checkins_locations[0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_checkins_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frequencies = df_checkins_locations.groupby(['userid', 'placeid'])[\"id\"].count().reset_index(name=\"frequency\")\n",
    "# df_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df_frequencies with df_locations using place_id and id fields\n",
    "df1 = pd.merge(df_frequencies, df_locations,left_on=\"placeid\",right_on=\"id\",how=\"inner\") \n",
    "df_frequencies = df1\n",
    "# df_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niPnBwMqaeIg"
   },
   "source": [
    "* Update the frequencies of *df_frequencies* to bring them back to the interval [0, 10] by applying normalization using tanx function.\n",
    "\n",
    "where $f_{min}$ and $f_{max}$ are respectively the minimum and maximum number of all the visit frequencies of any POI in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fmin\n",
    "f_min = df_frequencies['frequency'].min()\n",
    "# calculate f max\n",
    "f_max = df_frequencies['frequency'].max()\n",
    "# print(f_min, f_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.tanh = numpy tanh function\n",
    "df_frequencies[\"ratings\"] = df_frequencies[\"frequency\"].apply(lambda x: 10*np.tanh(10*(x-f_min)/(f_max-f_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_frequencies.head()\n",
    "# df_frequencies[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTkMe9tuaeL-"
   },
   "source": [
    "* Load *df_frequencies* into the *Suprise* framework using the *load_from_df()* function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0, 10))\n",
    "data = Dataset.load_from_df(df_frequencies[['userid', 'placeid', 'ratings']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkOr3KjSaeSX"
   },
   "source": [
    "*Use the *train_test_split()* function to split *df_frequencies* into a training set (*training set*, 75% of dataset) and a test set (*test set*, 25% of dataset) data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWzx_7igaeXa"
   },
   "source": [
    "*Associate each user with his list of POIs visited and place the result in a *dataframe* *df_user_POI*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_POI = df_frequencies.groupby('userid').placeid.apply(list).reset_index(name='POI_list') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_user_POI[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctLA38IhX_GK"
   },
   "source": [
    "# 3. Geographical Influence\n",
    "\n",
    "The *dataframe* *df_user_POI* associates each user $u$ with the list $L_u$ of POIs he has visited.\n",
    "\n",
    "\n",
    "* Use *df_user_POI* to calculate for each user $u$ the distances $d_{ij}$ between each pair of POIs visited: \n",
    "\n",
    "$\\forall p_i, p_j \\in L_u \\times L_u, d_{ij} = distance(p_i, p_j)$. \n",
    "\n",
    "We will denote this list of distances by $D_u$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCYoD9FXnwAT"
   },
   "outputs": [],
   "source": [
    "def distance(pi, pj):\n",
    "    \n",
    "    # retrieve first place latitude and longitude\n",
    "    lat0 = df_locations[df_locations.id == pi].lat.values\n",
    "    lng0 = df_locations[df_locations.id == pi].lng.values\n",
    "\n",
    "    # retrieve second place latitude and longitude\n",
    "    lat1 = df_locations[df_locations.id == pj].lat.values\n",
    "    lng1 = df_locations[df_locations.id == pj].lng.values\n",
    "\n",
    "    # calculate distance in km using the geopy library\n",
    "\n",
    "    return geodesic((lat0,lng0), (lat1,lng1)).km\n",
    "\n",
    "def distance_pair_list(Lu):\n",
    "    # collect all the combinations of pairs of places already visited\n",
    "    if len(Lu) <= 1 :\n",
    "        pairs = []\n",
    "    else:\n",
    "        pairs = list(combinations(Lu,2))\n",
    "        \n",
    "    \n",
    "    dist_list = []\n",
    "    \n",
    "    # calculate the distance between each pair of squares\n",
    "    if pairs != [] :\n",
    "        for pair in pairs :\n",
    "            dist_list.append(distance(pair[0], pair[1]))\n",
    "    return dist_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMuvQbc0aZfg"
   },
   "source": [
    "* Use $D_u$ to calculate the density $f_u$ of any distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPJJ-ZeDcbBA"
   },
   "source": [
    "The smoothing parameter $h = 1.06\\hat{\\sigma}n^{-1/5}$, where $n$ is the number of POIs present in $L_u$ and $\\hat{\\sigma}$ is the standard deviation of $D_u$. Implement the expression $\\hat{f}_u$ in a *density()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GC9aujQrng0W"
   },
   "outputs": [],
   "source": [
    "def k_gaussian(x):\n",
    "    return (1/np.sqrt(2*np.pi))*np.exp(-x**2/2)\n",
    "\n",
    "def density(Du, dij, h):\n",
    "    D2 = list(map(lambda x : (dij-x)/h,Du))\n",
    "    density = 1/(len(Du)*h)*sum(list(map(k_gaussian,D2)))\n",
    "    return density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUcipzKLc9bh"
   },
   "source": [
    "* The density $\\hat{f}_u$ is used to estimate the probability that a POI $p_i$ not visited by a user $u$ matches the geographic preferences of $u$ given his visit history. In order to obtain this probability, we calculate the distance between $p_i$ and each of the POIs of the list $L_u$ and we then estimate the probability of each of these distances passing through $\\hat{f}_u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG2EtVPHdQpp"
   },
   "source": [
    "* The final probability that a user $u$ visits a POI $p_i$ is obtained as follows:\n",
    "\n",
    "**P**(p_i \\| L_u) represents the probability that the user $u$ visits the POI $p_i$ given the geographical criterion. Implement the above equation in a *geo_proba()* function which takes as input the list $L_u$ and a POI, and which returns the probability of visiting this POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68uYwtgToV2D"
   },
   "outputs": [],
   "source": [
    "def geo_proba(Lu, pi, Du):\n",
    "    # pi= POI not visited by the user\n",
    "    # Lu = list of placeid of places visited by the user\n",
    "    # d is the list of distances btw pi and the places already visited by the user\n",
    "    # Du is the distance between each pair of places already visited\n",
    "    d = []\n",
    "    \n",
    "    # retrieve the longitude and latitude of the candidate place to recommend\n",
    "    lat_i = df_locations[df_locations.id==pi].lat.values[0]\n",
    "    long_i = df_locations[df_locations.id == pi].lng.values[0]\n",
    "    \n",
    "    # calculate all the distances between the candidate place to recommend and each place already visited in the Lu list\n",
    "    for l in Lu :\n",
    "        long_j = df_locations[df_locations.id == l].lng.values[0]\n",
    "        lat_j = df_locations[df_locations.id == l].lat.values[0]\n",
    "        d.append(geodesic((lat_j,long_j), (lat_i,long_i)).km)\n",
    "    \n",
    "    \n",
    "    # calculate h\n",
    "    n = len(Lu)\n",
    "    sigma = np.std(Du) #used to compute the standard deviation along the specified axis. This function returns the standard deviation of the array elements. The square root of the average square deviation (computed from the mean), is known as the standard deviation.\n",
    "    h = 1.06*sigma*n**(-1/5)\n",
    "    \n",
    "    # calculate density\n",
    "    density_list = list(map(lambda x : density(Du, x, h),d))\n",
    "\n",
    "    return np.mean(density_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIMqXOBIdjGI"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ua6RCpqeAuU"
   },
   "source": [
    "# 4. Social influence\n",
    "\n",
    "The *dataframe* *df_user_friends* associates each $u$ user with their $F(u)$ friends.\n",
    "\n",
    "* For each pair of users $(u, v)$, we calculate their *social similarity* using the Jaccard coefficient.\n",
    "\n",
    "Implement this coefficient in the *social_similarity()* function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFIYH0upodrz"
   },
   "outputs": [],
   "source": [
    "def social_similarity(u, v):\n",
    "    #friends of u\n",
    "    list1 = df_user_friends[df_user_friends.userid1==u].friends_list.values[0]\n",
    "    \n",
    "    # friends of v\n",
    "    list2 = df_user_friends[df_user_friends.userid1==v].friends_list.values[0]\n",
    "    sim = len(list(set(list1) & set(list2))) / len(list(set(list1) | set(list2)))\n",
    "    \n",
    "    #similarity is based on same friendhips rather than if they went to the same places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnOqZdzre-Yf"
   },
   "source": [
    "* This coefficient can be used in a collaborative filtering model.\n",
    "\n",
    "$r_{ui}$ indicates the visit frequency of $u$ in $i$, extracted from *df_frequencies*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rate(u,i,trainset):\n",
    "    #i= location id, u= user id\n",
    "    if(user_is_in_trainset(u) and item_is_in_trainset(i)):\n",
    "        inner_iid = trainset.to_inner_iid(i) # get the inner id of the location\n",
    "        inner_uid = trainset.to_inner_uid(u) # get the inner id of the user\n",
    "    else:\n",
    "        #not present in trainset\n",
    "        r = 0\n",
    "        return r\n",
    "    \n",
    "\n",
    "    res_ir = trainset.ir[inner_iid] # get rates given for the location, ir method returns item's ratings (ie list of ratings of an item fiven by different users. list is of tuples of form (user_inner_id, rating).\n",
    "    uid_ir = list(map(lambda x:x[0],res_ir)) #list of userid who have given rating to that location\n",
    "    rate_ir = list(map(lambda x:x[1],res_ir)) #list of ratings\n",
    "\n",
    "    if uid_ir.count(inner_uid) == 1: #ie if user u has rated item i\n",
    "        r = rate_ir[uid_ir.index(inner_uid)] \n",
    "    \n",
    "    # if the place has not been visited/rated by u\n",
    "    else:\n",
    "        r = 0\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_hat(user_i,j,trainset,F_i) :\n",
    "    #user_i = user id, j = location id, F_i = list of ids of friends of user with user_i\n",
    "    rate_j = np.zeros(len(F_i)) #used to generate an array containing zeros.\n",
    "    sim_list = np.zeros(len(F_i))\n",
    "    \n",
    "    F_i = enumerate(F_i)\n",
    "    \n",
    "    for i,user in F_i : #enumerate func converts a list into list of tuples where each tuple = (index, element)\n",
    "\n",
    "        rate_j[i] = get_rate(user,j,trainset)\n",
    "        sim_list[i] = social_similarity(user_i,user) #list of similarity scores of a user n his/her frnds => similarity btw 2 ppl depend on no. of their mutual frnds\n",
    "\n",
    "    return np.dot(sim_list,rate_j) / max(np.sum(sim_list),0.01) #returns the dot product of two arrays, used to compute the sum of all elements, the sum of each row, and the sum of each column of a given array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_is_in_trainset(u) :\n",
    "\n",
    "    try :\n",
    "        trainset.to_inner_uid(u)\n",
    "        res = 1\n",
    "    except :\n",
    "        res = 0\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_is_in_trainset(i) :\n",
    "\n",
    "    try :\n",
    "        trainset.to_inner_iid(i)\n",
    "        res = 1\n",
    "    except :\n",
    "        res = 0\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoPKIuyWokiX"
   },
   "outputs": [],
   "source": [
    "def social_proba(u, i, Lu):\n",
    "    # Lu = list of placeid of places visited by the user\n",
    "    #retrieve the list of all places\n",
    "    L = list(np.unique(df_locations.id))\n",
    "    # retrieve the list of places visited by u\n",
    "    # retrieve the list of places that have not yet been visited by user u\n",
    "    L_diff = list(set(L)-set(Lu))\n",
    "    # only consider locations that are in the training set\n",
    "    L_diff = [i for i in L_diff if item_is_in_trainset(i)]\n",
    "#     for i in L_diff:\n",
    "#         if (item_is_in_trainset(i)==0):\n",
    "#             L_diff.remove(i)\n",
    "    \n",
    "    # reduce the list of places to recommend to 50 due to computational complexity\n",
    "    L_diff = L_diff[:200]\n",
    "    # get u's friends list\n",
    "    F_i = df_user_friends[df_user_friends.userid1==u].friends_list.values[0] #F_i = list of ids friends of user with u\n",
    "    # consider only the list of friends who are in the training set\n",
    "    \n",
    "    for i in F_i:\n",
    "        if (user_is_in_trainset(i)==0):\n",
    "            F_i.remove(i)\n",
    "        \n",
    "    \n",
    "\n",
    "    if F_i == []:\n",
    "        return np.nan\n",
    "\n",
    "    else:\n",
    "\n",
    "        numerator = r_hat(u,i,trainset,F_i)\n",
    "        denominator = max(max(list(map(lambda x : r_hat(u,x,trainset,F_i),L_diff))),0.01)\n",
    "        \n",
    "        return numerator/denominator\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8G8Gd1jZgHUk"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmi7Y9CngIH9"
   },
   "source": [
    "# 5. Generation and evaluation of recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates top 8 recommendations based on highest rated places by a user's friends\n",
    "def frndRecom(u):\n",
    "    listFrnd = df_user_friends[df_user_friends.userid1==u].friends_list.values[0]\n",
    "    df_topRatedFrnd = df_frequencies[df_frequencies['userid'].isin(listFrnd)].sort_values('ratings',ascending=False)\n",
    "    return df_topRatedFrnd[0:8][\"placeid\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates top 8 recommendations based on highest rated places by all users (popular places)\n",
    "def topRatedRecom(u):\n",
    "    df_topRated = df_frequencies.sort_values('ratings',ascending=False)\n",
    "    return df_topRated[0:8][\"placeid\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates top 8 recommendations based on highest rated places by the user themselves in the past\n",
    "def topVisitedRecom(u):\n",
    "    df_topVisited = df_frequencies[df_frequencies.userid==u].sort_values('ratings',ascending=False)\n",
    "    return df_topVisited[0:8][\"placeid\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates 6 friends of user\n",
    "def Frnds(u):\n",
    "    listFrnd = df_user_friends[df_user_friends.userid1==u].friends_list.values[0]\n",
    "    return listFrnd[0:6]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates top 8 recommendations based on collaborative approach, using information about the geographical location of the place and social networks of the user.\n",
    "def genRecom(u):\n",
    "    \n",
    "    Lu = list(set(df_user_POI[df_user_POI.userid == u].POI_list.values[0])) #list of all the places visited by the user\n",
    "    L = list(np.unique(df_filtered_checkins.placeid)) # list of all the unique locations\n",
    "    L_diff = list(set(L)-set(Lu)) # list of all the places not visited by user\n",
    "    #making the list smaller because of computing reasons\n",
    "    L_diff=L_diff[0:200]\n",
    "    \n",
    "    data = {'score':[],\n",
    "        'placeid':[]}\n",
    "    scores = pd.DataFrame(data)\n",
    "    Du = distance_pair_list(Lu)\n",
    "    \n",
    "    for i in L_diff:\n",
    "        if np.isnan(social_proba(u, i, Lu)):\n",
    "                score = geo_proba(Lu, i, Du)   #return value of social_proba is nan when user has no frnds\n",
    "\n",
    "        else :\n",
    "            score = (geo_proba(Lu, i, Du) + social_proba(u, i, Lu)) / 2\n",
    "#         print(score)\n",
    "        df = pd.DataFrame({'score': [score], 'placeid': [i]})\n",
    "        scores = pd.concat([scores, df])\n",
    "\n",
    "\n",
    "    scores = scores.sort_values('score',ascending=False)\n",
    "    scores = scores[0:8] #return top 8 recommendations for user\n",
    "    return scores[\"placeid\"].tolist()\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the user id to a random number for all the recommendation generations. (Note: this random number should be present in the data)\n",
    "def recoms(u):\n",
    "    data={\n",
    "    \"frndRecom\" : frndRecom(u),\n",
    "    \"topRatedRecom\" : topRatedRecom(u),\n",
    "    \"topVisitedRecom\": topVisitedRecom(u),\n",
    "    \"genRecom\" : genRecom(u),\n",
    "    \"Frnds\":Frnds(u),\n",
    "    }\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO3ESMy9v84UQOmr2dpnnhI",
   "collapsed_sections": [],
   "name": "2_TP_POI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
